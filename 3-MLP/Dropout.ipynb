{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch \n",
    "from torch import nn\n",
    "from torchvision.datasets import MNIST"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout from scratch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 1., 0., 0., 0., 0., 0., 1., 1., 1.])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "(torch.rand(10) > 0.5).float()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "def dropout_layer(X, dropout):\n",
    "    assert 0 <= dropout <=1\n",
    "    if dropout == 1: return torch.zeros_like(X)\n",
    "    mask = (torch.rand(X.shape) > dropout).float()\n",
    "    return mask * X / (1.0 - dropout)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dropout concise implementation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preproccessing\n",
    "# Training data\n",
    "N_train, N_test = 5000, 10000\n",
    "mnist_train = MNIST(root='../data', train=True)\n",
    "X_train, targets_train = mnist_train.data.view(-1,784).float(), mnist_train.targets\n",
    "y_train = torch.zeros((len(targets_train),10))\n",
    "for i in range(len(targets_train)):\n",
    "    y_train[i, targets_train[i]] = 1\n",
    "\n",
    "# test data\n",
    "mnist_test = MNIST(root='../data', train=False)\n",
    "X_test, targets_test = mnist_test.data.view(-1,784).float(), mnist_test.targets\n",
    "y_test = torch.zeros((len(targets_test),10))\n",
    "for i in range(len(targets_test)):\n",
    "    y_test[i, targets_test[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model\n",
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(MLP, self).__init__()\n",
    "        self.linear1 = nn.Linear(784, 128)\n",
    "        self.linear2 = nn.Linear(128, 64)\n",
    "        self.linear3 = nn.Linear(64, 10)\n",
    "    def forward(self, x):\n",
    "        h = nn.functional.relu(self.linear1(x))\n",
    "        h = nn.functional.dropout(h, p=0.5, training=self.training)\n",
    "        h = nn.functional.relu(self.linear2(h))\n",
    "        h = nn.functional.dropout(h, p=0.5, training=self.training)\n",
    "        y = nn.functional.softmax(self.linear3(h))\n",
    "        # y = self.linear3(h)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1l/s_0rbq615qb6cfhphk3n17km0000gn/T/ipykernel_49941/1226839732.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y = nn.functional.softmax(self.linear3(h))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20/2000], Loss: 1.7500\n",
      "Epoch: [40/2000], Loss: 1.6558\n",
      "Epoch: [60/2000], Loss: 1.6095\n",
      "Epoch: [80/2000], Loss: 1.5690\n",
      "Epoch: [100/2000], Loss: 1.5483\n",
      "Epoch: [120/2000], Loss: 1.5377\n",
      "Epoch: [140/2000], Loss: 1.5301\n",
      "Epoch: [160/2000], Loss: 1.5251\n",
      "Epoch: [180/2000], Loss: 1.5195\n",
      "Epoch: [200/2000], Loss: 1.5149\n",
      "Epoch: [220/2000], Loss: 1.5130\n",
      "Epoch: [240/2000], Loss: 1.5106\n",
      "Epoch: [260/2000], Loss: 1.5083\n",
      "Epoch: [280/2000], Loss: 1.5064\n",
      "Epoch: [300/2000], Loss: 1.5052\n",
      "Epoch: [320/2000], Loss: 1.5034\n",
      "Epoch: [340/2000], Loss: 1.5027\n",
      "Epoch: [360/2000], Loss: 1.5014\n",
      "Epoch: [380/2000], Loss: 1.5008\n",
      "Epoch: [400/2000], Loss: 1.4981\n",
      "Epoch: [420/2000], Loss: 1.4986\n",
      "Epoch: [440/2000], Loss: 1.4974\n",
      "Epoch: [460/2000], Loss: 1.4955\n",
      "Epoch: [480/2000], Loss: 1.4957\n",
      "Epoch: [500/2000], Loss: 1.4950\n",
      "Epoch: [520/2000], Loss: 1.4938\n",
      "Epoch: [540/2000], Loss: 1.4936\n",
      "Epoch: [560/2000], Loss: 1.4938\n",
      "Epoch: [580/2000], Loss: 1.4922\n",
      "Epoch: [600/2000], Loss: 1.4931\n",
      "Epoch: [620/2000], Loss: 1.4915\n",
      "Epoch: [640/2000], Loss: 1.4915\n",
      "Epoch: [660/2000], Loss: 1.4904\n",
      "Epoch: [680/2000], Loss: 1.4899\n",
      "Epoch: [700/2000], Loss: 1.4902\n",
      "Epoch: [720/2000], Loss: 1.4894\n",
      "Epoch: [740/2000], Loss: 1.4896\n",
      "Epoch: [760/2000], Loss: 1.4893\n",
      "Epoch: [780/2000], Loss: 1.4888\n",
      "Epoch: [800/2000], Loss: 1.4885\n",
      "Epoch: [820/2000], Loss: 1.4887\n",
      "Epoch: [840/2000], Loss: 1.4877\n",
      "Epoch: [860/2000], Loss: 1.4876\n",
      "Epoch: [880/2000], Loss: 1.4877\n",
      "Epoch: [900/2000], Loss: 1.4867\n",
      "Epoch: [920/2000], Loss: 1.4870\n",
      "Epoch: [940/2000], Loss: 1.4862\n",
      "Epoch: [960/2000], Loss: 1.4868\n",
      "Epoch: [980/2000], Loss: 1.4863\n",
      "Epoch: [1000/2000], Loss: 1.4863\n",
      "Epoch: [1020/2000], Loss: 1.4863\n",
      "Epoch: [1040/2000], Loss: 1.4859\n",
      "Epoch: [1060/2000], Loss: 1.4854\n",
      "Epoch: [1080/2000], Loss: 1.4853\n",
      "Epoch: [1100/2000], Loss: 1.4849\n",
      "Epoch: [1120/2000], Loss: 1.4854\n",
      "Epoch: [1140/2000], Loss: 1.4844\n",
      "Epoch: [1160/2000], Loss: 1.4842\n",
      "Epoch: [1180/2000], Loss: 1.4841\n",
      "Epoch: [1200/2000], Loss: 1.4836\n",
      "Epoch: [1220/2000], Loss: 1.4837\n",
      "Epoch: [1240/2000], Loss: 1.4838\n",
      "Epoch: [1260/2000], Loss: 1.4835\n",
      "Epoch: [1280/2000], Loss: 1.4844\n",
      "Epoch: [1300/2000], Loss: 1.4831\n",
      "Epoch: [1320/2000], Loss: 1.4841\n",
      "Epoch: [1340/2000], Loss: 1.4831\n",
      "Epoch: [1360/2000], Loss: 1.4827\n",
      "Epoch: [1380/2000], Loss: 1.4832\n",
      "Epoch: [1400/2000], Loss: 1.4825\n",
      "Epoch: [1420/2000], Loss: 1.4829\n",
      "Epoch: [1440/2000], Loss: 1.4823\n",
      "Epoch: [1460/2000], Loss: 1.4830\n",
      "Epoch: [1480/2000], Loss: 1.4822\n",
      "Epoch: [1500/2000], Loss: 1.4824\n",
      "Epoch: [1520/2000], Loss: 1.4826\n",
      "Epoch: [1540/2000], Loss: 1.4824\n",
      "Epoch: [1560/2000], Loss: 1.4814\n",
      "Epoch: [1580/2000], Loss: 1.4819\n",
      "Epoch: [1600/2000], Loss: 1.4826\n",
      "Epoch: [1620/2000], Loss: 1.4815\n",
      "Epoch: [1640/2000], Loss: 1.4824\n",
      "Epoch: [1660/2000], Loss: 1.4818\n",
      "Epoch: [1680/2000], Loss: 1.4822\n",
      "Epoch: [1700/2000], Loss: 1.4821\n",
      "Epoch: [1720/2000], Loss: 1.4816\n",
      "Epoch: [1740/2000], Loss: 1.4818\n",
      "Epoch: [1760/2000], Loss: 1.4823\n",
      "Epoch: [1780/2000], Loss: 1.4818\n",
      "Epoch: [1800/2000], Loss: 1.4816\n",
      "Epoch: [1820/2000], Loss: 1.4817\n",
      "Epoch: [1840/2000], Loss: 1.4820\n",
      "Epoch: [1860/2000], Loss: 1.4812\n",
      "Epoch: [1880/2000], Loss: 1.4810\n",
      "Epoch: [1900/2000], Loss: 1.4807\n",
      "Epoch: [1920/2000], Loss: 1.4813\n",
      "Epoch: [1940/2000], Loss: 1.4812\n",
      "Epoch: [1960/2000], Loss: 1.4808\n",
      "Epoch: [1980/2000], Loss: 1.4806\n",
      "Epoch: [2000/2000], Loss: 1.4799\n"
     ]
    }
   ],
   "source": [
    "# Config model, criterion and optimizer\n",
    "model = MLP()\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.Adam(model.parameters())\n",
    "\n",
    "# Traning\n",
    "num_epochs = 2000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    \n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred,y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1)%20 == 0:\n",
    "        print(f'Epoch: [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 96.80%\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/1l/s_0rbq615qb6cfhphk3n17km0000gn/T/ipykernel_49941/1226839732.py:15: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n",
      "  y = nn.functional.softmax(self.linear3(h))\n"
     ]
    }
   ],
   "source": [
    "# Prediction\n",
    "model.training = False\n",
    "y_hat = model(X_test)\n",
    "target_hat = torch.max(y_hat, dim=-1)[1]\n",
    "acc = torch.sum(targets_test == target_hat)/N_test\n",
    "print(f\"Test accuracy: {acc * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
