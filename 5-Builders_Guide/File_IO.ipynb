{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional as F"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = torch.arange(4)\n",
    "torch.save(x, 'x-file')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0, 1, 2, 3])"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x2 = torch.load('x-file')\n",
    "x2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]), tensor([0., 0., 0., 0.]))"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y = torch.zeros(4)\n",
    "torch.save([x,y], 'x-files')\n",
    "x2, y2 = torch.load('x-files')\n",
    "(x2, y2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([0, 1, 2, 3]),\n",
       " {'x': tensor([0, 1, 2, 3]), 'y': tensor([0., 0., 0., 0.])})"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mydict = {'x':x, 'y':y}\n",
    "torch.save(mydict, 'mydict')\n",
    "mydict2 = torch.load('mydict')\n",
    "mydict2['x'], mydict2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayx/miniforge3/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    }
   ],
   "source": [
    "class MLP(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.hidden = nn.LazyLinear(256)\n",
    "        self.output = nn.LazyLinear(10)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.output(F.relu(self.hidden(x)))\n",
    "\n",
    "net = MLP()\n",
    "X = torch.randn(size=(2, 20))\n",
    "Y = net(X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "OrderedDict([('hidden.weight',\n",
       "              tensor([[-0.2159,  0.0872,  0.0975,  ...,  0.2057,  0.1578,  0.0286],\n",
       "                      [-0.0805,  0.1570, -0.1246,  ..., -0.0318,  0.0363,  0.0303],\n",
       "                      [ 0.0841,  0.0896,  0.1910,  ...,  0.1310,  0.1123, -0.2233],\n",
       "                      ...,\n",
       "                      [-0.0297,  0.0268, -0.2232,  ...,  0.1779,  0.2209, -0.0070],\n",
       "                      [-0.1829, -0.1700, -0.1952,  ..., -0.1971,  0.0909,  0.2071],\n",
       "                      [-0.0765,  0.0498,  0.1159,  ...,  0.0032, -0.0610, -0.0458]])),\n",
       "             ('hidden.bias',\n",
       "              tensor([-0.1700,  0.1111,  0.1403, -0.1061,  0.1650,  0.1942, -0.0579,  0.0213,\n",
       "                      -0.0350, -0.1911, -0.0708, -0.0396, -0.0474,  0.1998,  0.1579, -0.1230,\n",
       "                      -0.1582, -0.1575,  0.1851,  0.1642,  0.0698, -0.1192, -0.1635,  0.1193,\n",
       "                       0.0471, -0.1660,  0.2153,  0.1845, -0.1903,  0.0669,  0.1488,  0.0753,\n",
       "                      -0.1439,  0.2003,  0.1403,  0.1562, -0.0369, -0.0918,  0.0423, -0.2148,\n",
       "                      -0.0052,  0.0626, -0.1515,  0.1411,  0.1408,  0.1485,  0.2034,  0.1552,\n",
       "                       0.1018, -0.0970,  0.1404,  0.2215,  0.1373,  0.0072,  0.1923,  0.0190,\n",
       "                       0.1809,  0.1334,  0.0801, -0.1596,  0.1171, -0.1413, -0.1723,  0.0893,\n",
       "                      -0.0501, -0.1496,  0.0271, -0.1832,  0.1972, -0.1893, -0.1132,  0.0622,\n",
       "                      -0.0454,  0.0463, -0.0850, -0.0700,  0.1743,  0.1450, -0.1916, -0.1164,\n",
       "                       0.2019,  0.0686,  0.2074,  0.1407, -0.0142, -0.0985, -0.0710,  0.1955,\n",
       "                       0.0757, -0.1483, -0.1736, -0.0769, -0.1824,  0.0205, -0.0693, -0.0202,\n",
       "                       0.0706,  0.2208, -0.1253,  0.2210,  0.0349,  0.2132,  0.0790,  0.1674,\n",
       "                      -0.2100, -0.0761,  0.0172, -0.1528,  0.0380,  0.1310, -0.0179,  0.0611,\n",
       "                      -0.0628, -0.1643, -0.0471,  0.1326, -0.0187, -0.1336, -0.0098, -0.0631,\n",
       "                      -0.0253, -0.1742, -0.2081,  0.1914,  0.1337, -0.1172, -0.0322, -0.1350,\n",
       "                       0.2066,  0.1333, -0.0922,  0.1960, -0.1044, -0.2025, -0.1702, -0.0042,\n",
       "                      -0.1197,  0.0438,  0.0227,  0.0565, -0.1041, -0.1976, -0.0166,  0.2107,\n",
       "                      -0.0702,  0.1796, -0.1195,  0.1680, -0.0628, -0.1702,  0.0319,  0.1585,\n",
       "                      -0.0971,  0.0909,  0.0295,  0.1869, -0.0431,  0.0158, -0.0565, -0.0273,\n",
       "                       0.0008, -0.1439, -0.1449,  0.1200, -0.1876,  0.0787, -0.0208, -0.0316,\n",
       "                      -0.1748, -0.0731,  0.1079, -0.0120,  0.0532,  0.1855,  0.0802,  0.0703,\n",
       "                      -0.1254,  0.1245, -0.0637,  0.1862, -0.2052,  0.1990,  0.0791, -0.1465,\n",
       "                      -0.2211,  0.0271,  0.2018,  0.0712,  0.2079,  0.1513, -0.1907, -0.0039,\n",
       "                       0.1347, -0.0287, -0.1547,  0.0266, -0.0466, -0.0423, -0.0108,  0.0311,\n",
       "                       0.0267,  0.1812, -0.1106, -0.0726, -0.1016,  0.1456, -0.1187, -0.0032,\n",
       "                      -0.0843,  0.1944, -0.1783, -0.1907,  0.2042, -0.0975,  0.1463,  0.1784,\n",
       "                       0.1188, -0.0442,  0.1466,  0.1565,  0.1100, -0.1236, -0.0925, -0.0429,\n",
       "                      -0.1081, -0.1300, -0.1572, -0.0822, -0.0489,  0.1227,  0.1009, -0.1006,\n",
       "                      -0.1721,  0.1015, -0.0772,  0.0740, -0.0911, -0.2203, -0.1365, -0.2141,\n",
       "                       0.1633, -0.1393,  0.1070,  0.0968,  0.1662, -0.0950, -0.0088,  0.2233,\n",
       "                       0.2094, -0.1027, -0.0841,  0.0386,  0.1002,  0.0127,  0.1275, -0.0573])),\n",
       "             ('output.weight',\n",
       "              tensor([[-0.0040, -0.0226, -0.0566,  ...,  0.0599,  0.0515,  0.0376],\n",
       "                      [-0.0613, -0.0005, -0.0395,  ..., -0.0412, -0.0251, -0.0225],\n",
       "                      [-0.0567,  0.0021, -0.0415,  ...,  0.0164,  0.0504, -0.0520],\n",
       "                      ...,\n",
       "                      [-0.0528, -0.0448,  0.0600,  ...,  0.0351,  0.0040, -0.0303],\n",
       "                      [-0.0064, -0.0381, -0.0127,  ..., -0.0295, -0.0144, -0.0323],\n",
       "                      [-0.0285, -0.0442,  0.0550,  ..., -0.0546,  0.0413,  0.0085]])),\n",
       "             ('output.bias',\n",
       "              tensor([ 0.0506,  0.0329, -0.0509,  0.0282, -0.0027, -0.0312, -0.0207, -0.0328,\n",
       "                       0.0100, -0.0600]))])"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "net.state_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(net.state_dict(), 'mlp.params')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/jayx/miniforge3/lib/python3.10/site-packages/torch/nn/modules/lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "MLP(\n",
       "  (hidden): LazyLinear(in_features=0, out_features=256, bias=True)\n",
       "  (output): LazyLinear(in_features=0, out_features=10, bias=True)\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "clone = MLP()\n",
    "clone.load_state_dict(torch.load('mlp.params'))\n",
    "clone.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[True, True, True, True, True, True, True, True, True, True],\n",
       "        [True, True, True, True, True, True, True, True, True, True]])"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "Y_clone = clone(X)\n",
    "Y_clone == Y"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
