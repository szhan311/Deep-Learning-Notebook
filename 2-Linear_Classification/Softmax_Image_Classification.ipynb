{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import random\n",
    "from torchvision.datasets import MNIST\n",
    "from torchvision.datasets import FashionMNIST\n",
    "from torchvision import transforms\n",
    "import time\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Download and Formatting the data\n",
    "Write the code for downloading and formatting the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Preproccessing\n",
    "# Training data\n",
    "N_train, N_test = 5000, 10000\n",
    "mnist_train = MNIST(root='./data', train=True)\n",
    "X_train, targets_train = mnist_train.data.view(-1,784).float(), mnist_train.targets\n",
    "y_train = torch.zeros((len(targets_train),10))\n",
    "for i in range(len(targets_train)):\n",
    "    y_train[i, targets_train[i]] = 1\n",
    "\n",
    "# test data\n",
    "mnist_test = MNIST(root='./data', train=False)\n",
    "X_test, targets_test = mnist_test.data.view(-1,784).float(), mnist_test.targets\n",
    "y_test = torch.zeros((len(targets_test),10))\n",
    "for i in range(len(targets_test)):\n",
    "    y_test[i, targets_test[i]] = 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trans = transforms.Compose([transforms.Resize((32,32)), transforms.ToTensor()])\n",
    "# Fashin_MNIST_train = FashionMNIST(root = './data', train = True, transform=trans, download=True)\n",
    "# Fashin_MNIST_test = FashionMNIST(root = './data', train = True, transform=trans, download=True)\n",
    "# def text_labels(indices):\n",
    "#       labels = ['t-shirt', 'trouser', 'pullover', 'dress', 'coat',\n",
    "#               'sandal', 'shirt', 'sneaker', 'bag', 'ankle boot']\n",
    "#       return [labels[int(i)] for i in indices]\n",
    "\n",
    "# # test\n",
    "# indices = [1, 3, 6]\n",
    "# text_labels(indices)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Classifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def softmax(X):\n",
    "    X_exp = torch.exp(X)\n",
    "    partition = X_exp.sum(axis=1, keepdim=True)\n",
    "    return X_exp/ partition\n",
    "\n",
    "# cross entropy\n",
    "def cross_entropy(y_hat, y):\n",
    "    return -torch.sum(y * torch.log(y_hat))\n",
    "\n",
    "# def cross_entropy(y_hat, y):\n",
    "#     return -torch.log(y_hat[list(range(len(y_hat))), y]).mean()\n",
    "\n",
    "y = torch.tensor([0, 1])\n",
    "y_hat = softmax(torch.tensor([[100, -100]]))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# define the model\n",
    "class Linear_Regression(torch.nn.Module):\n",
    "    def __init__(self, input_dim = 784, output_dim=10):\n",
    "        super(Linear_Regression, self).__init__()\n",
    "        self.linear = torch.nn.Linear(input_dim, output_dim)\n",
    "    def forward(self, x):\n",
    "        h = self.linear(x)\n",
    "        return torch.nn.functional.softmax(h)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Linear_Regression()\n",
    "model = nn.Sequential(\n",
    "    nn.Linear(784, 10),\n",
    "    nn.Softmax(dim=1)\n",
    ")\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = torch.optim.SGD(model.parameters(), lr = 5e-3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: [20/2000], Loss: 1.8501\n",
      "Epoch: [40/2000], Loss: 1.8112\n",
      "Epoch: [60/2000], Loss: 1.8019\n",
      "Epoch: [80/2000], Loss: 1.7978\n",
      "Epoch: [100/2000], Loss: 1.7951\n",
      "Epoch: [120/2000], Loss: 1.7926\n",
      "Epoch: [140/2000], Loss: 1.7910\n",
      "Epoch: [160/2000], Loss: 1.7901\n",
      "Epoch: [180/2000], Loss: 1.7894\n",
      "Epoch: [200/2000], Loss: 1.7886\n",
      "Epoch: [220/2000], Loss: 1.7881\n",
      "Epoch: [240/2000], Loss: 1.7876\n",
      "Epoch: [260/2000], Loss: 1.7871\n",
      "Epoch: [280/2000], Loss: 1.7867\n",
      "Epoch: [300/2000], Loss: 1.7864\n",
      "Epoch: [320/2000], Loss: 1.7863\n",
      "Epoch: [340/2000], Loss: 1.7860\n",
      "Epoch: [360/2000], Loss: 1.7858\n",
      "Epoch: [380/2000], Loss: 1.7857\n",
      "Epoch: [400/2000], Loss: 1.7870\n",
      "Epoch: [420/2000], Loss: 1.7852\n",
      "Epoch: [440/2000], Loss: 1.7845\n",
      "Epoch: [460/2000], Loss: 1.7841\n",
      "Epoch: [480/2000], Loss: 1.7838\n",
      "Epoch: [500/2000], Loss: 1.7837\n",
      "Epoch: [520/2000], Loss: 1.7837\n",
      "Epoch: [540/2000], Loss: 1.7835\n",
      "Epoch: [560/2000], Loss: 1.7833\n",
      "Epoch: [580/2000], Loss: 1.7832\n",
      "Epoch: [600/2000], Loss: 1.7834\n",
      "Epoch: [620/2000], Loss: 1.7829\n",
      "Epoch: [640/2000], Loss: 1.7827\n",
      "Epoch: [660/2000], Loss: 1.7826\n",
      "Epoch: [680/2000], Loss: 1.7824\n",
      "Epoch: [700/2000], Loss: 1.7826\n",
      "Epoch: [720/2000], Loss: 1.7827\n",
      "Epoch: [740/2000], Loss: 1.7823\n",
      "Epoch: [760/2000], Loss: 1.7815\n",
      "Epoch: [780/2000], Loss: 1.7811\n",
      "Epoch: [800/2000], Loss: 1.7809\n",
      "Epoch: [820/2000], Loss: 1.7806\n",
      "Epoch: [840/2000], Loss: 1.7804\n",
      "Epoch: [860/2000], Loss: 1.7803\n",
      "Epoch: [880/2000], Loss: 1.7802\n",
      "Epoch: [900/2000], Loss: 1.7811\n",
      "Epoch: [920/2000], Loss: 1.7800\n",
      "Epoch: [940/2000], Loss: 1.7796\n",
      "Epoch: [960/2000], Loss: 1.7794\n",
      "Epoch: [980/2000], Loss: 1.7793\n",
      "Epoch: [1000/2000], Loss: 1.7797\n",
      "Epoch: [1020/2000], Loss: 1.7793\n",
      "Epoch: [1040/2000], Loss: 1.7792\n",
      "Epoch: [1060/2000], Loss: 1.7793\n",
      "Epoch: [1080/2000], Loss: 1.7791\n",
      "Epoch: [1100/2000], Loss: 1.7789\n",
      "Epoch: [1120/2000], Loss: 1.7790\n",
      "Epoch: [1140/2000], Loss: 1.7788\n",
      "Epoch: [1160/2000], Loss: 1.7794\n",
      "Epoch: [1180/2000], Loss: 1.7793\n",
      "Epoch: [1200/2000], Loss: 1.7790\n",
      "Epoch: [1220/2000], Loss: 1.7784\n",
      "Epoch: [1240/2000], Loss: 1.7784\n",
      "Epoch: [1260/2000], Loss: 1.7786\n",
      "Epoch: [1280/2000], Loss: 1.7782\n",
      "Epoch: [1300/2000], Loss: 1.7781\n",
      "Epoch: [1320/2000], Loss: 1.7781\n",
      "Epoch: [1340/2000], Loss: 1.7780\n",
      "Epoch: [1360/2000], Loss: 1.7780\n",
      "Epoch: [1380/2000], Loss: 1.7780\n",
      "Epoch: [1400/2000], Loss: 1.7780\n",
      "Epoch: [1420/2000], Loss: 1.7782\n",
      "Epoch: [1440/2000], Loss: 1.7780\n",
      "Epoch: [1460/2000], Loss: 1.7786\n",
      "Epoch: [1480/2000], Loss: 1.7795\n",
      "Epoch: [1500/2000], Loss: 1.7779\n",
      "Epoch: [1520/2000], Loss: 1.7775\n",
      "Epoch: [1540/2000], Loss: 1.7773\n",
      "Epoch: [1560/2000], Loss: 1.7773\n",
      "Epoch: [1580/2000], Loss: 1.7774\n",
      "Epoch: [1600/2000], Loss: 1.7773\n",
      "Epoch: [1620/2000], Loss: 1.7772\n",
      "Epoch: [1640/2000], Loss: 1.7772\n",
      "Epoch: [1660/2000], Loss: 1.7771\n",
      "Epoch: [1680/2000], Loss: 1.7773\n",
      "Epoch: [1700/2000], Loss: 1.7776\n",
      "Epoch: [1720/2000], Loss: 1.7776\n",
      "Epoch: [1740/2000], Loss: 1.7773\n",
      "Epoch: [1760/2000], Loss: 1.7767\n",
      "Epoch: [1780/2000], Loss: 1.7766\n",
      "Epoch: [1800/2000], Loss: 1.7765\n",
      "Epoch: [1820/2000], Loss: 1.7765\n",
      "Epoch: [1840/2000], Loss: 1.7768\n",
      "Epoch: [1860/2000], Loss: 1.7764\n",
      "Epoch: [1880/2000], Loss: 1.7764\n",
      "Epoch: [1900/2000], Loss: 1.7765\n",
      "Epoch: [1920/2000], Loss: 1.7763\n",
      "Epoch: [1940/2000], Loss: 1.7765\n",
      "Epoch: [1960/2000], Loss: 1.7767\n",
      "Epoch: [1980/2000], Loss: 1.7762\n",
      "Epoch: [2000/2000], Loss: 1.7763\n"
     ]
    }
   ],
   "source": [
    "num_epochs = 2000\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    y_pred = model(X_train)\n",
    "    loss = criterion(y_pred,y_train)\n",
    "    optimizer.zero_grad()\n",
    "    loss.backward()\n",
    "    optimizer.step()\n",
    "    if (epoch + 1)%20 == 0:\n",
    "        print(f'Epoch: [{epoch + 1}/{num_epochs}], Loss: {loss.item():.4f}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test accuracy: 67.54%\n"
     ]
    }
   ],
   "source": [
    "y_hat = model(X_test)\n",
    "target_hat = torch.max(y_hat, dim=-1)[1]\n",
    "acc = torch.sum(targets_test == target_hat)/N_test\n",
    "print(f\"Test accuracy: {acc * 100:.2f}%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "hnn",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.15"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
